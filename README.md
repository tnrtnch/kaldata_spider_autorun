# Kaldata Scraper ğŸ•·ï¸
<br />
This project is a Scrapy-based web scraping application that extracts news articles from Kaldata.<br />
The collected data is stored in a SQLite database (kaldata.db) and automatically updated every hour via GitHub Actions.<br />
<br />
# Features ğŸš€
<br />
Extracts article title, publication date, author, and body using Scrapy<br />
Stores all scraped data into a SQLite database (kaldata.db)<br />
Automated crawling with GitHub Actions (runs every hour)<br />
Database updates are version-controlled via Git commits<br />
<br />
# Project Structure ğŸ“‚
**kaldata-scraper**
â”‚<br />
â”œâ”€â”€ **.github/workflows/**             # GitHub Actions workflows<br />
â”‚ &nbsp; &nbsp;  â””â”€â”€ **scrapy-crawl.yml**<br />
â”œâ”€â”€ **kaldata**/                      # Scrapy project package<br />
â”‚  &nbsp; &nbsp; â”œâ”€â”€ **content-schema.json**        # Item definitions<br />
â”‚  &nbsp; &nbsp; â”œâ”€â”€ **content.json**               # Item definitions<br />
â”‚  &nbsp; &nbsp;  â”œâ”€â”€ **items.py**                   # Item definitions<br />
â”‚  &nbsp; &nbsp;  â”œâ”€â”€ **kaldata.db**                 # SQLite database (auto-created/updated)<br />
â”‚  &nbsp; &nbsp;  â”œâ”€â”€ **main.py**                    # <br />
â”‚  &nbsp; &nbsp;  â”œâ”€â”€ **middlewares.py**             # <br />
â”‚  &nbsp; &nbsp;  â”œâ”€â”€ **pipelines.py**               # SQLite pipeline<br />
â”‚  &nbsp; &nbsp;  â”œâ”€â”€ **settings.py**                # Scrapy settings<br />
â”‚  &nbsp; &nbsp;  â””â”€â”€ **spiders/**<br />
â”‚  &nbsp; &nbsp; &nbsp; &nbsp;      â””â”€â”€ **kaldata_spider.py**      # Main spider<br />
â”‚<br />
â”œâ”€â”€ **flowchart_kaldata_spider.jpg**   # Workflow<br />
â”œâ”€â”€ **scrapy.cfg**                     # Scrapy configuration file<br />
â”œâ”€â”€ **requirements.txt**               # Requirements<br />
â””â”€â”€ **README.md**<br />
<br />

Local Setup âš™ï¸ <br />
<br />
Clone the repository:<br />
git clone https://github.com/<your-user_name>/kaldata-scraper.git<br />
cd kaldata-scraper<br />
Install dependencies:<br />
pip install scrapy<br />
Run the spider:<br />
scrapy crawl kaldata_spider<br />
After execution, a kaldata.db SQLite database will be created/updated with the scraped articles.<br />
<br />
# GitHub Actions (Automated Runs) â° <br />
<br />
This project includes a workflow at .github/workflows/scrapy-crawl.yml.<br />
The workflow:<br />
<br />
Runs every hour (via cron job)<br />
Executes the spider<br />
Updates kaldata.db with new data<br />
Commits changes back to the repository<br />
You can view run logs under the Actions tab in the repository.<br />
<br />
# Database Schema ğŸ“ <br />
<br />
The SQLite database contains a single table: articles<br />
<br />
Field	Type	Description<br />
id	int	Auto-increment PK<br />
title	text	Article title<br />
pubdate	text	Publication datetime<br />
author	text	Article author<br />
body	text	Article content<br />
# ContributingğŸ”§ <br />
<br />
Issues and pull requests are welcome.<br />
Future improvements could include exporting to JSON/CSV or adding more categories.<br />
